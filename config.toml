# Server configuration
[server]
# HTTP server port
port = 8125
# HTTP server host address
host = "0.0.0.0"
# URL of the frontend application
frontend_url = "${LOGCHEF_SERVER__FRONTEND_URL:-http://localhost:8125}"
# HTTP server timeout for requests
http_server_timeout = "30s"

# SQLite database configuration
[sqlite]
# Path to the SQLite database file
path = "local.db"

# OpenID Connect configuration
[oidc]
# URL of the OIDC provider for discovery
provider_url = "${LOGCHEF_OIDC__PROVIDER_URL:-http://localhost:5556/dex}"
# Authentication endpoint URL
auth_url = "${LOGCHEF_OIDC__AUTH_URL:-http://localhost:5556/dex/auth}"
# Token endpoint URL
token_url = "${LOGCHEF_OIDC__TOKEN_URL:-http://localhost:5556/dex/token}"
# Client ID for OIDC authentication
client_id = "${LOGCHEF_OIDC__CLIENT_ID:-logchef}"
# Client secret for OIDC authentication
client_secret = "${LOGCHEF_OIDC__CLIENT_SECRET:-logchef-secret}"
# Callback URL for OIDC authentication
redirect_url = "${LOGCHEF_OIDC__REDIRECT_URL:-http://localhost:8125/api/v1/auth/callback}"
# OIDC scopes to request
scopes = ["openid", "email", "profile"]

# Authentication configuration
[auth]
# Email addresses of admin users
admin_emails = ["admin@logchef.internal"]
# Duration of user sessions
session_duration = "8h"
# Maximum number of concurrent sessions per user
max_concurrent_sessions = 1
# Secret key for API token hashing (generate with: openssl rand -hex 32)
api_token_secret = "5679649c50fddda837449b77d9983ab5f8dba65878897e968a74e7061bf47677"
# Default API token expiration (empty for no expiration)
default_token_expiry = "2160h"  # 90 days = 90 * 24 = 2160 hours

# Logging configuration
[logging]
# Log level (debug, info, warn, error)
level = "debug"

# AI SQL generation configuration
[ai]
# Enable/disable AI features
enabled = true
# API endpoint
# Base URL for OpenAI API (leave empty for default OpenAI endpoint)
base_url = "https://openrouter.ai/api/v1"
# OpenAI API key for AI SQL generation
api_key = ""
# Model parameters
# Model to use (default: gpt-4o)
model = "gpt-4o"
# Maximum tokens to generate
max_tokens = 1024
# Temperature for generation (0.0-1.0, lower is more deterministic)
temperature = 0.1
